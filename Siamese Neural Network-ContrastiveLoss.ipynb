{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necesssary packages\n",
    "%matplotlib inline\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import PIL.ImageOps    \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img,text=None,should_save=False):\n",
    "    npimg = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic',fontweight='bold',\n",
    "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelling similar and dissimilar image pairs\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self,root_dir,imageFolderDataset, transform=None, should_invert=True):\n",
    "        self.root_dir=root_dir\n",
    "        self.imageFolderDataset=imageFolderDataset\n",
    "        self.transform=transform\n",
    "        self.should_invert=should_invert\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        should_get_same_class=random.randint(0,1)\n",
    "        \n",
    "        img1_tuple=random.choice(self.imageFolderDataset.imgs)\n",
    "        if should_get_same_class:\n",
    "            while True:\n",
    "                img2_tuple=random.choice(self.imageFolderDataset.imgs)\n",
    "                if img1_tuple[1]==img2_tuple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            while True:\n",
    "                img2_tuple=random.choice(self.imageFolderDataset.imgs)\n",
    "                if img1_tuple[1]!=img2_tuple[1]:\n",
    "                    break\n",
    "        \n",
    "        img1=Image.open(img1_tuple[0])\n",
    "        img2=Image.open(img2_tuple[0])\n",
    "        \n",
    "        if self.should_invert:\n",
    "            img1=PIL.ImageOps.invert(img1)\n",
    "            img2=PIL.ImageOps.invert(img2)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img1=self.transform(img1)\n",
    "            img2=self.transform(img2)\n",
    "            \n",
    "        return img1, img2, torch.from_numpy(np.array([(img1_tuple[1]!=img2_tuple[1])], dtype=np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define image size, no of epochs and batch size\n",
    "img_size=105 \n",
    "epochs=50\n",
    "batchsize=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir='./data/Omniglot/train/'\n",
    "train_imagefolder=dset.ImageFolder(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=SiameseDataset(root_dir=train_dir, imageFolderDataset=train_imagefolder,\n",
    "                             transform=transforms.Compose([transforms.Resize((img_size,img_size)),\n",
    "                                                           transforms.RandomHorizontalFlip(),\n",
    "                                                           transforms.RandomResizedCrop(img_size, scale=(0.8,1.0)),\n",
    "                                                           transforms.ToTensor()\n",
    "                                                          ]),\n",
    "                             should_invert=False)\n",
    "\n",
    "train_dataloader=DataLoader(train_dataset, batch_size=batchsize, num_workers=0, shuffle=True)\n",
    "\n",
    "print(\"Train dataset has {} images\\n\".format(len(train_dataset)))\n",
    "print(\"Number of batches in train dataset with batch size of {} is {}\".format(train_dataloader.batch_size,len(train_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize how images in a batch are labelled as similar and dissimilar\n",
    "visualize_dataloader = DataLoader(train_dataset,\n",
    "                        shuffle=True,\n",
    "                        num_workers=0,\n",
    "                        batch_size=8)\n",
    "dataiter = iter(visualize_dataloader)\n",
    "\n",
    "\n",
    "example_batch = next(dataiter)\n",
    "concatenated = torch.cat((example_batch[0],example_batch[1]),0)\n",
    "imshow(torchvision.utils.make_grid(concatenated))\n",
    "print(example_batch[2].numpy()) #same == 0  #different == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Koch et al.   \n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 10),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 7),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 128, 4),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 4),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.dropout=nn.Dropout(p=0.5) \n",
    "        \n",
    "        self.fc1=nn.Sequential(\n",
    "            nn.Linear(9216, 4096),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "        self.out=nn.Linear(4096,1)\n",
    "        \n",
    "    def forward_once(self,inp):\n",
    "        inp=self.conv(inp)\n",
    "        inp=inp.view(inp.size()[0], -1)\n",
    "        inp = self.dropout(inp) #\n",
    "        inp=self.fc1(inp)\n",
    "        return inp\n",
    "        \n",
    "    def forward(self, inp1, inp2):\n",
    "        out1=self.forward_once(inp1)\n",
    "        out2=self.forward_once(inp2)\n",
    "        return out1,out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no of parameters in the network\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(SiameseNetwork())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if CUDA is available, set device to CUDA\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of CUDA devices available :  {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU Name : {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No CUDA devices found\")\n",
    "    \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define contrastive loss\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set loss criterion and optimizer\n",
    "net=SiameseNetwork().cuda()\n",
    "criterion=ContrastiveLoss()\n",
    "optimizer=optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dir='./data/Omniglot/valid/'\n",
    "valid_imagefolder=dset.ImageFolder(valid_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset=SiameseDataset(root_dir=valid_dir, imageFolderDataset=valid_imagefolder,\n",
    "                             transform=transforms.Compose([transforms.Resize((img_size,img_size)),\n",
    "                                                           transforms.RandomHorizontalFlip(),\n",
    "                                                           transforms.RandomResizedCrop(img_size, scale=(0.8,1.0)),\n",
    "                                                           transforms.ToTensor()\n",
    "                                                          ]),\n",
    "                             should_invert=False)\n",
    "\n",
    "valid_dataloader=DataLoader(valid_dataset, batch_size=batchsize, num_workers=0, shuffle=True)\n",
    "\n",
    "print(\"Validation dataset has {} images\\n\".format(len(valid_dataset)))\n",
    "print(\"Number of batches in validation dataset with batch size of {} is {}\".format(valid_dataloader.batch_size,len(valid_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_dataloader, valid_dataloader, epochs, criterion):\n",
    "    train_loss=[] # training loss for every epoch\n",
    "    valid_loss=[] # validation loss for every epoch\n",
    "    sum_train_loss=0.0 # sum of training losses for every epoch\n",
    "    sum_valid_loss=0.0 # sum of validation losses for every epoch\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_epoch_loss=0.0\n",
    "        net.train()\n",
    "        for i, data in enumerate(train_dataloader,0):\n",
    "            img1, img2, label = data\n",
    "            img1, img2, label = img1.cuda(), img2.cuda(), label.cuda()\n",
    "            \n",
    "            label = label.float()\n",
    "            \n",
    "            output1, output2 = net(img1, img2)\n",
    "            loss = criterion(output1, output2, label)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_epoch_loss = train_epoch_loss + ((1/(i+1)) * (loss.item() - train_epoch_loss))\n",
    "            \n",
    "        train_loss.append(train_epoch_loss)\n",
    "        sum_train_loss+=train_epoch_loss\n",
    "        \n",
    "        valid_epoch_loss=0.0\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            for i, data in enumerate(valid_dataloader,0):\n",
    "                img1, img2, label = data\n",
    "                img1, img2, label = img1.cuda(), img2.cuda(), label.cuda()\n",
    "                \n",
    "                output1, output2 = net(img1, img2)\n",
    "                loss = criterion(output1, output2, label)\n",
    "                \n",
    "                valid_epoch_loss = valid_epoch_loss + ((1/(i+1)) * (loss.item() - valid_epoch_loss))\n",
    "                \n",
    "        valid_loss.append(valid_epoch_loss)\n",
    "        sum_valid_loss+=valid_epoch_loss\n",
    "        \n",
    "        print(\"Epoch {}/{}\\n Train loss : {} \\t Valid loss {}\\n\"\n",
    "             .format(epoch, epochs, train_epoch_loss, valid_epoch_loss))\n",
    "        \n",
    "    print(\"Average training loss after {} epochs : {}\".format(epochs, sum_train_loss/epochs))\n",
    "    print(\"Average validation loss after {} epochs : {}\".format(epochs, sum_valid_loss/epochs))\n",
    "    \n",
    "    return train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation\n",
    "train_losses, valid_losses = train(net, train_dataloader, valid_dataloader, epochs, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training loss and validation loss\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(train_losses, label=\"Train loss\")\n",
    "plt.plot(valid_losses, label=\"Validation loss\")\n",
    "plt.legend(bbox_to_anchor=(1.1,1.0), loc='upper left')\n",
    "plt.savefig('plot.png',dpi=300,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir='./data/Omniglot/test/'\n",
    "test_imagefolder=dset.ImageFolder(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset=SiameseDataset(root_dir=test_dir, imageFolderDataset=test_imagefolder,\n",
    "                             transform=transforms.Compose([transforms.Resize((img_size,img_size)),\n",
    "                                                           transforms.RandomHorizontalFlip(),\n",
    "                                                           transforms.RandomResizedCrop(img_size, scale=(0.8,1.0)),\n",
    "                                                           transforms.ToTensor()\n",
    "                                                          ]),\n",
    "                             should_invert=False)\n",
    "\n",
    "test_dataloader=DataLoader(test_dataset, batch_size=1, num_workers=0, shuffle=True)\n",
    "\n",
    "print(\"Test dataset has {} images\\n\".format(len(test_dataset)))\n",
    "print(\"Number of batches in test dataset with batch size of {} is {}\".format(test_dataloader.batch_size,len(test_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(net, test_dataloader):\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        count=100\n",
    "        correct=0\n",
    "        correct_count=0\n",
    "        dataiter = iter(test_dataloader)\n",
    "        print(\"Testing...\")\n",
    "        for i in range(count):\n",
    "            img1,img2,label=next(dataiter)\n",
    "            cat=torch.cat((img1, img2),0)\n",
    "            output1, output2 = net(Variable(img1).cuda(), Variable(img2).cuda()) \n",
    "            euclidean_distance = F.pairwise_distance(output1, output2) \n",
    "            prediction = torch.sigmoid(euclidean_distance)\n",
    "            total = label.size(0)\n",
    "            \n",
    "            # check if prediction and actual label are same\n",
    "            for j in range(label.size(0)):\n",
    "                if (prediction[j]>0.5) and (label[j]==1):\n",
    "                    correct+=1\n",
    "                elif (prediction[j]<0.5) and (label[j]==0):\n",
    "                    correct+=1\n",
    "                \n",
    "            correct_count+=correct/total\n",
    "            correct=0\n",
    "            imshow(torchvision.utils.make_grid(cat),'Pred : {:.2f} Label : {}'.format(prediction.item(),label.item()))      \n",
    "    return correct_count, count, (correct_count/count)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "acc=eval(net, test_dataloader)\n",
    "print('{} correct predictions out of {}\\nAccuracy : {:.2f}'.format(acc[0],acc[1], acc[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-siamese",
   "language": "python",
   "name": "env-siamese"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
